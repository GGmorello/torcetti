#!/usr/bin/env python3
"""
End-to-end training script demonstrating Torcetti's capabilities.
This script trains a simple neural network classifier on synthetic data.
"""

import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# Import Torcetti components
from torcetti.core.tensor import Tensor
from torcetti.nn.linear import Linear
from torcetti.nn.sequential import Sequential
from torcetti.nn.activations import ReLU, Softmax
from torcetti.loss.crossentropy import CrossEntropyLoss
from torcetti.optim.sgd import SGD
from torcetti.optim.adam import Adam


def generate_spiral_data(n_samples=300, n_classes=3, noise=0.1):
    """Generate synthetic spiral dataset for classification."""
    np.random.seed(42)
    
    X = np.zeros((n_samples * n_classes, 2))
    y = np.zeros(n_samples * n_classes, dtype=int)
    
    for class_idx in range(n_classes):
        # Generate spiral points
        r = np.linspace(0.1, 1, n_samples)
        t = np.linspace(class_idx * 4, (class_idx + 1) * 4, n_samples) + np.random.randn(n_samples) * noise
        
        start_idx = class_idx * n_samples
        end_idx = (class_idx + 1) * n_samples
        
        X[start_idx:end_idx, 0] = r * np.cos(t)
        X[start_idx:end_idx, 1] = r * np.sin(t)
        y[start_idx:end_idx] = class_idx
    
    return X.astype(np.float32), y


def create_model(input_size, hidden_size, num_classes):
    """Create a simple neural network classifier."""
    model = Sequential(
        Linear(input_size, hidden_size),
        ReLU(),
        Linear(hidden_size, hidden_size),
        ReLU(),
        Linear(hidden_size, num_classes)
    )
    return model


def train_epoch(model, X_train, y_train, optimizer, criterion, batch_size=32):
    """Train for one epoch with mini-batches."""
    n_samples = len(X_train)
    n_batches = (n_samples + batch_size - 1) // batch_size
    
    total_loss = 0.0
    correct_predictions = 0
    
    # Shuffle training data
    indices = np.random.permutation(n_samples)
    
    for batch_idx in range(n_batches):
        # Get batch indices
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, n_samples)
        batch_indices = indices[start_idx:end_idx]
        
        # Create batch tensors
        X_batch = Tensor(X_train[batch_indices], requires_grad=False)
        y_batch = Tensor(y_train[batch_indices].astype(np.float32), requires_grad=False)
        
        # Forward pass
        predictions = model(X_batch)
        loss = criterion(predictions, y_batch)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Track metrics
        total_loss += loss.data
        predicted_classes = np.argmax(predictions.data, axis=1)
        correct_predictions += np.sum(predicted_classes == y_batch.data)
    
    avg_loss = total_loss / n_batches
    accuracy = correct_predictions / n_samples
    return avg_loss, accuracy


def evaluate(model, X_test, y_test):
    """Evaluate the model on test data."""
    X_tensor = Tensor(X_test, requires_grad=False)
    predictions = model(X_tensor)
    
    predicted_classes = np.argmax(predictions.data, axis=1)
    accuracy = np.mean(predicted_classes == y_test)
    
    return accuracy, predictions.data


def plot_results(X, y, model, title="Decision Boundary"):
    """Plot the dataset and learned decision boundary."""
    plt.figure(figsize=(10, 8))
    
    # Create a mesh for decision boundary
    h = 0.02
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                        np.arange(y_min, y_max, h))
    
    # Get model predictions on mesh
    mesh_points = np.c_[xx.ravel(), yy.ravel()].astype(np.float32)
    mesh_tensor = Tensor(mesh_points, requires_grad=False)
    mesh_predictions = model(mesh_tensor)
    mesh_classes = np.argmax(mesh_predictions.data, axis=1)
    mesh_classes = mesh_classes.reshape(xx.shape)
    
    # Plot decision boundary
    plt.contourf(xx, yy, mesh_classes, alpha=0.3, cmap=plt.cm.RdYlBu)
    
    # Plot data points
    colors = ['red', 'blue', 'green']
    for class_idx in range(3):
        mask = y == class_idx
        plt.scatter(X[mask, 0], X[mask, 1], c=colors[class_idx], 
                   label=f'Class {class_idx}', alpha=0.7, s=20)
    
    plt.xlabel('X1')
    plt.ylabel('X2')
    plt.title(title)
    plt.legend()
    plt.grid(True, alpha=0.3)


def main():
    """Main training script."""
    print("ðŸš€ Torcetti End-to-End Training Demo")
    print("=" * 50)
    
    # Hyperparameters
    n_samples = 200
    n_classes = 3
    hidden_size = 64
    learning_rate = 0.01
    n_epochs = 100
    batch_size = 32
    
    print(f"Generating spiral dataset: {n_samples * n_classes} samples, {n_classes} classes")
    
    # Generate data
    X, y = generate_spiral_data(n_samples, n_classes)
    
    # Split data (80% train, 20% test)
    n_total = len(X)
    n_train = int(0.8 * n_total)
    
    indices = np.random.permutation(n_total)
    train_indices = indices[:n_train]
    test_indices = indices[n_train:]
    
    X_train, y_train = X[train_indices], y[train_indices]
    X_test, y_test = X[test_indices], y[test_indices]
    
    print(f"Train set: {len(X_train)} samples")
    print(f"Test set: {len(X_test)} samples")
    
    # Create model
    print(f"\nCreating neural network:")
    print(f"  Architecture: {X.shape[1]} -> {hidden_size} -> {hidden_size} -> {n_classes}")
    
    model = create_model(X.shape[1], hidden_size, n_classes)
    
    # Count parameters
    total_params = sum(p.data.size for p in model.parameters())
    print(f"  Total parameters: {total_params:,}")
    
    # Loss and optimizer
    criterion = CrossEntropyLoss()
    
    # Try both SGD and Adam
    optimizers = {
        'SGD': SGD(model.parameters(), lr=learning_rate, momentum=0.9),
        'Adam': Adam(model.parameters(), lr=learning_rate)
    }
    
    for opt_name, optimizer in optimizers.items():
        print(f"\nðŸ”§ Training with {opt_name} optimizer")
        print("-" * 30)
        
        # Reset model parameters
        model = create_model(X.shape[1], hidden_size, n_classes)
        optimizer = optimizers[opt_name].__class__(model.parameters(), lr=learning_rate)
        if opt_name == 'SGD':
            optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9)
        
        # Training history
        train_losses = []
        train_accuracies = []
        
        # Training loop
        for epoch in range(n_epochs):
            train_loss, train_acc = train_epoch(model, X_train, y_train, optimizer, criterion, batch_size)
            
            train_losses.append(train_loss)
            train_accuracies.append(train_acc)
            
            if (epoch + 1) % 20 == 0:
                test_acc, _ = evaluate(model, X_test, y_test)
                print(f"Epoch {epoch+1:3d}/{n_epochs} | "
                      f"Loss: {train_loss:.4f} | "
                      f"Train Acc: {train_acc:.3f} | "
                      f"Test Acc: {test_acc:.3f}")
        
        # Final evaluation
        test_accuracy, test_predictions = evaluate(model, X_test, y_test)
        print(f"\nðŸ“Š Final Results ({opt_name}):")
        print(f"  Train Accuracy: {train_accuracies[-1]:.3f}")
        print(f"  Test Accuracy: {test_accuracy:.3f}")
        
        # Plot results
        plt.figure(figsize=(15, 5))
        
        # Training curves
        plt.subplot(1, 3, 1)
        plt.plot(train_losses)
        plt.title(f'Training Loss ({opt_name})')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 3, 2)
        plt.plot(train_accuracies)
        plt.title(f'Training Accuracy ({opt_name})')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.grid(True, alpha=0.3)
        
        # Decision boundary
        plt.subplot(1, 3, 3)
        plot_results(X, y, model, f'Decision Boundary ({opt_name})')
        
        plt.tight_layout()
        plt.savefig(f'training_results_{opt_name.lower()}.png', dpi=150, bbox_inches='tight')
        print(f"  ðŸ“ˆ Plots saved as 'training_results_{opt_name.lower()}.png'")
        
        plt.show()
    
    print("\nâœ… Training completed successfully!")
    print("\nðŸŽ¯ Key achievements:")
    print("  âœ“ Automatic differentiation working")
    print("  âœ“ Neural network layers functional")
    print("  âœ“ Loss functions computing correctly")
    print("  âœ“ Optimizers updating parameters")
    print("  âœ“ End-to-end gradient flow verified")
    print("\nðŸš€ Torcetti is ready for real-world applications!")


if __name__ == "__main__":
    main() 